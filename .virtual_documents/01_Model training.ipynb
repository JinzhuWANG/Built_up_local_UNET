import torch
import torch.nn as nn
from torch.utils.data import DataLoader,Dataset

from tqdm import tqdm

# from torchvision.utils import save_image
from sklearn.model_selection import train_test_split





device = 'cuda' if torch.cuda.is_available() else 'cpu'

NUM_EPOCH = 400
IN_CHANNLE = 40
BATCH_SIZE = 12





# define the region for traning
target = 'xinan'
year = '2017_2019'

# define the year-value of built-up img
year_val = dict(zip([f'{i}_{i+2}' for i in range(1990,2020,3)],range(10,0,-1)))
year_val














from torch_modules.UNET_model import UNET





model = UNET()

# change the input_layer to match the input data
model.downs[0] = nn.Sequential(
                nn.Conv2d(IN_CHANNLE, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU())

model.to(device)
print()








# read X_y_path_df_filter 
X_y_path_df = pd.read_csv('../02_Data_on_disk/X_y_path_df.csv')
X_y_path_df








# dataset class
class X_y_dataset(Dataset):
    def __init__(self,X_y_df):
        self.X_y_df = X_y_df
        
    def __getitem__(self,index):
        row = self.X_y_df.iloc[index]
        X_path,y_path = row.tolist()
        
        # read X data
        with GzipFile(X_path,'r') as f:
            X_arry = np.load(f,allow_pickle=True)
        X_tensor = torch.FloatTensor(X_arry)
        # read y data
        with GzipFile(y_path,'r') as f:
            y_arry = np.load(f,allow_pickle=True)
        y_tensor = torch.FloatTensor(y_arry)

        # reclassify the y_tensor to match its year mapping
        y_tensor = torch.where(y_tensor>year_val[year],1,0)
        
        return X_tensor,y_tensor
    
    def __len__(self):
        return len(self.X_y_df)
            
            





# random split the X_y_df
Sample_df = X_y_path_df.sample(SAMPLE_SIZE)

train_df,test_df = train_test_split(Sample_df,test_size=0.2)

train_dataset = X_y_dataset(train_df)
train_data_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,pin_memory=True,num_workers=12)

test_dataset = X_y_dataset(test_df)
test_data_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE, pin_memory=True, num_workers=12)











# 1) define loss function
criterion = torch.nn.BCELoss()

# 2) difine optimizer
optimizer = torch.optim.Adam(model.parameters())





# function to evaluate the model
def eval_model(model,loader):

    # set to eval mode
    model.eval()

    # compute losses
    losses = []
    with torch.no_grad():
        for data in tqdm(loader,total=len(loader)):

            # get data, then send them to GPU
            x = data[0].float().to(device)
            y = data[1].squeeze(1).float().to(device)

            # train the model
            score = model(x).squeeze(1)

            # compute loss
            loss = criterion(score,y)
            losses.append(loss.detach().cpu().item())
 
    # change model back to training mode    
    model.train()

    return losses








def pred_one_img(img_path = '../03_Model_training/Train_model_pred_imgs'):
    # load one img
    x_arry = np.load(f'{img_path}/X_array.npy',allow_pickle=True)
    x_img = torch.FloatTensor(x_arry).unsqueeze(0).to(device)

    y_arry = np.load(f'{img_path}/y_array.npy',allow_pickle=True)

    # passing the img to model
    pred_img = model(x_img).cpu().detach().numpy()[0,0,:,:]
    true_img = y_arry[0]

    # save true-pred img to disk
    concat_img = np.hstack([pred_img,true_img])
    save_image(torch.tensor(concat_img),f"{img_path}/train_pred_img_{epoch:03}.jpeg")








# load the trained modle
longest_trained_model = glob(f'../03_Model_training/Saved_models/{region}_{year}_progress_model*')
if len(longest_trained_model) > 0:

    # load historical training models
    longest_trained_model = sorted(longest_trained_model)[-1]
    model.load_state_dict(torch.load(longest_trained_model,map_location=torch.device(device)))
    start_epoch = int(longest_trained_model[-7:-4])

    metrics_df = pd.read_csv('../03_Model_training/Metrics_csv/metrics.csv',header=None)
    best_loss = metrics_df[metrics_df[1]=='eval'][2].min()
else:
    start_epoch = 0
    best_loss = 1e9
    # empty the recored folders
    files = [i for i in glob('../03_Model_training/Saved_models/*')]
    files.extend(glob('../03_Model_training/Metrics_csv/*'))
    files.extend(glob('../03_Model_training/Train_model_pred_imgs/*.jpeg'))
    for i in files:
        os.remove(i)
        print(f'{i} has been deleted!')








for epoch in range(start_epoch,NUM_EPOCH+1):
    
    train_losses = []
    for data in tqdm(train_data_loader,total=len(train_data_loader)):
        # model(data)
        X_data = data[0].float().to(device)
        y = data[1].squeeze(1).float().to(device)

        # prediction
        y_hat = model(X_data).squeeze(1)

        # compute loss
        loss = criterion(y_hat,y)

        # automatically update model parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # record the tranin loss
        train_losses.append(loss.detach().cpu().item())
    
    # compute the avg. train loss
    train_losses_mean = np.array(train_losses).mean()

    # compute the avg. test loss
    test_losses = eval_model(model,test_data_loader)
    test_losses_mean = np.array(test_losses).mean()

    # write metric to disk
    with open('../03_Model_training/Metrics_csv/metrics.csv', 'a') as f:
        f.write(f'{epoch},train,{train_losses_mean}\n')
        f.write(f'{epoch},eval,{test_losses_mean}\n')

    # pred a picture and save it to disk
    pred_one_img()

    # save models to disk
    model_path = '../03_Model_training/Saved_models'

    # save best models
    if test_losses_mean < best_loss:
        # update best_loss
        best_loss = test_losses_mean
        # save model to disk
        torch.save(model.state_dict(), f'{model_path}/{region}_{year}_best_model_{epoch:03}.tar')

    # save model for every 5 eopch
    if (epoch)%5 == 0:
        # save model to disk
        torch.save(model.state_dict(), f'{model_path}/{region}_{year}_progress_model_{epoch:03}.tar')


    # report the training process
    print(f"The loss of 【{epoch:03}】 is: Train:{train_losses_mean:.5f} Eval:{test_losses_mean:.5f}\n")














# read csv
loss_df = pd.read_csv('./Metrics_csv/metrics.csv',header=None)

# set headers
loss_df.columns = ['epoch','type','val']





# plot
sns.lineplot(loss_df >> mask(X.val < 0.1),x='epoch',y='val',hue='type')





















